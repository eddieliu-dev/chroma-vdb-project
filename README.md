# Chroma VDB + LLM Document Processing

This project combines a local **Chroma vector database** with a **local LLM API (via Ollama)** to process and query text documents. It performs document embedding, title and keyword extraction, and supports semantic search via natural language queries.

---

## ğŸ“‚ Project Structure

```
chroma-vdb-project/
â”‚
â”œâ”€â”€ chroma/                     # Chroma DB setup and operations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chroma_client.py        # Initializes Chroma DB client
â”‚   â””â”€â”€ chroma_function.py      # CRUD operations for Chroma
â”‚
â”œâ”€â”€ chroma_data/                # Persistent storage for Chroma
â”‚
â”œâ”€â”€ data/                       # Raw input documents
â”‚   â”œâ”€â”€ documents_dup_part_1_part_1
â”‚   â””â”€â”€ documents_dup_part_1_part_1_short
â”‚
â”œâ”€â”€ llm/                        # LLM API and embedding utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_client.py           # Sends prompt to local LLM (Ollama)
â”‚   â””â”€â”€ llm_embedding.py        # Embedding function using Langchain + Ollama
â”‚
â”œâ”€â”€ service/                    # Empty for future improvements
â”‚
â”œâ”€â”€ main.py                     # Main entry point
â”œâ”€â”€ prompt.py                   # Prompt templates for LLM
â”œâ”€â”€ record_helper.py            # Metadata and document ID utilities
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ .venv/                      # Virtual environment (excluded from Git)
```

---

## ğŸš€ Features

- âœ… Extracts embeddings from raw text using `bge-m3` model
- ğŸ§  Uses a local LLM (`llama3.1:latest`) to extract:
  - ğŸ”¹ Short titles
  - ğŸ”¹ 3â€“5 keywords
- ğŸ—ƒï¸ Stores data in Chroma with metadata (UUID, creator, timestamp)
- ğŸ” Supports semantic search based on user query

---

## ğŸ› ï¸ Setup

1. **Install dependencies**  
   Use `poetry` or `pip` to install:
   ```bash
   pip install chromadb langchain aiohttp
   ```

2. **Start Ollama LLM Server**  
   Install and start Ollama:
   ```bash
   ollama run llama3
   ```

3. **Run the project**
   ```bash
   python main.py
   ```

---

## ğŸ“˜ How It Works

- When you run `main.py`, you can choose:
  - `Add`: reads text, generates embeddings, metadata, stores in Chroma
  - `Query`: asks a semantic question, returns best-matching document

- The prompts used:
  - Title prompt: generates a short (â‰¤10 characters) summary
  - Keywords prompt: extracts 3â€“5 topic-relevant keywords

---

## ğŸ“„ Example Query

```text
æœ‰æ²¡æœ‰ç¾å›½ç›¸å…³çš„æ–°é—»?
```

Returns:
- Matched title  
- Extracted keywords  
- Full document text

---

## âœ¨ Future Improvements

- [ ] Add multi-language support
- [ ] Add more service features